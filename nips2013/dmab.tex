\documentclass{article} % For LaTeX2e
\usepackage{nips13submit_e,times}
\usepackage{hyperref}
\usepackage{url}
%\documentstyle[nips13submit_09,times,art10]{article} % For LaTeX 2.09

% For citations
\usepackage[numbers]{natbib}
%\renewcommand{\citet}{\cite}

% For algorithms
\usepackage{algorithm,algorithmic}
% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
%\newcommand{\theHalgorithm}{\arabic{algorithm}}

% for wrapping text around algorithms
\usepackage{wrapfig}

% kill spaces in enumerate
\usepackage{enumitem}
\setlist{nolistsep}
\setlist[itemize]{leftmargin=4ex}

% more stuff
\usepackage{multirow}
\usepackage{cellspace}
\include{header}

%\sloppy
%\nipsfinalcopy % Uncomment for camera-ready version
%\gdef\isconf{1} % Uncomment for short version



% notations
\newcommand{\phat}{\hat{p}}
\newcommand{\qhat}{\hat{q}}
\newcommand{\Dele}{\Del^{\eps}}
\newcommand{\Delst}{\Del_{\st}}
\newcommand{\cA}{c_\mathcal{A}}




%\title{Exploratory Multi-Player Multi-Armed Bandits}
\title{Distributed Exploration in Multi-Armed Bandits}


\author{
Eshcar Hillel\\
Yahoo! Labs, Haifa\\
%Address \\
\texttt{eshcar@yahoo-inc.com} \\
\And
Zohar Karnin \\
Yahoo! Labs, Haifa\\
%Address \\
\texttt{zkarnin@yahoo-inc.com} \\
\And
Tomer Koren%
\thanks{Most of this work was done while the author was at Yahoo! Research.} \\
Technion --- Israel Institute of Technology \\
%Address \\
\texttt{tomerk@technion.ac.il} \\
\And
Ronny Lempel \\
Yahoo! Labs, Haifa\\
%Address \\
\texttt{rlempel@yahoo-inc.com} \\
\And
Oren Somekh \\
Yahoo! Labs, Haifa\\
%Address \\
\texttt{orens@yahoo-inc.com} \\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.




\begin{document} 
\maketitle



\begin{abstract}
We study exploration in Multi-Armed Bandits (MAB) in a setting where~$k$ players collaborate in order to identify an $\eps$-optimal arm.
Our motivation comes from recent employment of MAB algorithms in computationally intensive, large-scale applications.
Our results demonstrate a non-trivial tradeoff between the number of arm pulls required by each of the players, and the amount of communication between them.
In particular, our main result shows that by allowing the $k$ players to communicate \emph{only once}, they are able to learn $\sqrt{k}$ times faster than a single player.
That is, distributing learning to $k$ players gives rise to a factor~$\sqrt{k}$ parallel speed-up.
We complement this result with a lower bound showing this is in general the best possible. 
On the other extreme, we present an algorithm that achieves the ideal factor $k$ speed-up in learning performance, with communication only logarithmic in~$1/\eps$. 
\end{abstract}



%\input{intro.tex}
\vspace{-1ex}
\section{Introduction}
\label{sec:intro}
\vspace{-1ex}

Over the past years, multi-armed bandit (MAB) algorithms have been employed in an increasing amount of large-scale applications.
MAB algorithms rank results of search engines~\citep{InterleavedBuckets, Dueling2009}, choose between stories or ads to showcase on web sites~\citep{COKE2008,COKE-mortal2008}, accelerate model selection and stochastic optimization tasks \citep{maron1994hoeffding,mnih2008empirical}, and more.
In many of these applications, the workload is simply too high to be handled by a single processor.
%websites cannot be served from a single front-end machine.
In the web context, for example, the sheer volume of user requests and the high rate at which they arrive, require websites to use many front-end machines that run in multiple data centers.
In the case of model selection tasks, a single evaluation of a certain model or configuration might require considerable computation time, so that distributing the exploration process across several nodes may result with a significant gain in performance. 
%In this paper, we study such large-scale MAB problems where distributed computation is a must for practical implementations.
In this paper, we study such large-scale MAB problems in a distributed environment where 
learning is performed by several independent nodes that may take actions and observe rewards in parallel.
%where distributed computation is a must for practical implementations.

Following recent MAB literature~\cite{audibert10,evendar06}, we focus on the problem of identifying a ``good'' bandit arm with high confidence. 
In this problem, we may repeatedly choose one arm (corresponding to an action) and observe a reward drawn from a probability distribution associated with that arm.
Our goal is to find an arm with an (almost) optimal expected reward, with as few arm pulls as possible (that is, minimize the \emph{simple regret}~\cite{bubeck2009pure}).
Our objective is thus explorative in nature, and in particular we do not care about the incurred costs or the involved regret. 
This is indeed the natural goal in many applications, such as in the case of model selection problems mentioned above.
In our setup, a distributed strategy is evaluated by the number of arm pulls \emph{per node} required for the task, which correlates with the parallel speed-up obtained by distributing the learning process.

We abstract a distributed MAB system as follows.
In our model, there are $k$ \emph{players} that correspond to $k$ independent machines in a cluster. 
The players are presented with a set of arms, with a common goal of identifying a good arm.
Each player receives a stream of queries upon each it chooses an arms to pull. 
This stream is usually regulated by some load balancer ensuring the load is roughly divided evenly across players.
To collaborate, the players may communicate with each other.
We assume that the bandwidth of the underlying network is limited, so that players cannot simply share every piece of information.
Also, communicating over the network might incur substantial latencies, so players should refrain from doing so as much as possible. 
When measuring communication of a certain multi-player protocol we consider the number of \emph{communication rounds} it requires, where in a round of communication each player broadcasts a single message to all other players. 
%This is equivalent to considering a synchronous message-passing model~\cite{AttiyaDC2004}.
%\footnote{A synchronous model simplifies our analysis and is reasonable as a first step in a new distributed setting.}. 
%Round-based models are common in the distributed learning literature (e.g., \cite{balcan12,daume2012efficient}).
%Round-based models are natural in distributed frameworks such as \emph{MapReduce}~\cite{MapReduce08} in which players communicate through files, in a way that can be captured by a synchronous \emph{shared-memory} model~\cite{AttiyaDC2004}.
%Likewise for MapReduce, more rounds means more jobs which utilize more system resources, and therefore should be optimized.
Round-based models are natural in distributed learning scenarios, where frameworks such as MapReduce~\cite{MapReduce08} are ubiquitous.

What is the tradeoff between the learning performance of the players, and the communication between them?
At one extreme, if all players broadcast to each other each and every arm reward as it is observed, they can simply simulate the decisions of a serial, optimal algorithm. 
However, the communication load of this strategy is of course prohibitive. 
At the other extreme, if the players never communicate, each will suffer the learning curve of a single player, thereby avoiding any possible speed-up the distributed system may provide.
Our goal in this work is to better understand this tradeoff between inter-player communication and learning performance.

%Recalling our motivating scenarios in which communication is extremely costly,
Considering the high cost of communication, perhaps the simplest and most important question that arises is how well can the players learn while keeping communication to the very minimum.
%between two consecutive communication rounds, without sharing any information throughout the process.
More specifically, is there a non-trivial strategy by which the players can identify a ``good'' arm while communicating only once, at the end of the process?
As we discuss later on, this is a non-trivial question.
On the positive side, we present a $k$-player algorithm that attains an asymptotic parallel speed-up of $\sqrt{k}$ factor, as compared to the conventional, serial setting.
% that needs only $\sqrt{k}$ times more arm pulls for identifying an~$\eps$-best bandit arm, as compared to the conventional, serial setting.
%This implies that distributing the learning process over $k$ players can lead to an asymptotic parallel speed-up of $\sqrt{k}$ factor.
In fact, our approach demonstrates how to convert virtually any serial exploration strategy to a distributed algorithm enjoying such speed-up.
Ideally, one could hope for a factor $k$ speed-up in learning performance; however, we show a lower bound on the required number of pulls in this case, implying that our $\sqrt{k}$ speed-up is essentially optimal.
%On the negative side, we complement our algorithmic results for this case with a lower bound on the required number of pulls, implying that our $\sqrt{k}$ speed-up is essentially optimal. 

At the other end of the trade-off, we investigate how much communication is necessary for obtaining the ideal factor $k$ parallel speed-up. 
%In addition, we investigate how many communication rounds are required for competing with the learning performance of a single-player algorithm.
We present a $k$-player strategy achieving such speed-up, with communication only logarithmic in~$1/\eps$.
%We present a $k$-player strategy achieving the same learning performance (in terms of number of arms pulls needed) as a serial algorithm, that performs only~$\log(1/\eps)$ communication rounds. \tk{don't mention rounds}
As a corollary, we derive an algorithm that demonstrates an explicit trade-off between the number of arm pulls and the amount of inter-player communication.





\subsection{Related Work}
%\vspace{-1ex}


Recently there has been an increasing interest in distributed and collaborative learning problems.
In the MAB literature, several recent works consider multi-player MAB scenarios in which players actually \emph{compete} with each other, either on arm-pulls resources~\cite{gabillon2011multi} or on the rewards received~\cite{liu2010distributed}.
In contrast, we study a \emph{collaborative} multi-player problem and investigate how sharing observations helps players achieve their common goal.
The related work of \citet{kanade2012distributed} in the context of non-stochastic (i.e.~adversarial) experts also deals with a collaborative problem in a similar distributed setup, and examine the trade-off between communication and the cumulative regret.

Another line of recent work was focused on distributed stochastic optimization~\cite{duchi2010distributed,agarwal2011delayed,dekel12} and distributed PAC models~\cite{balcan12,daume2012protocols,daume2012efficient}, investigating the involved communication trade-offs. 
The techniques developed there, however, are inherently ``batch'' learning methods and thus are not directly applicable to our MAB problem which is online in nature.
Questions involving network topology~\cite{duchi2010distributed,dekel12} and delays~\cite{agarwal2011delayed} are relevant to our setup as well; however, our present work focuses on establishing the first non-trivial guarantees in a distributed collaborative MAB setting.





\section{Problem Setup and Statement of Results}
\label{sec:prelims}
%\vspace{-1ex}


In our model of the Distributed Multi-Armed Bandit problem, there are $k \ge 1$ individual players. The players are given $n$ arms, enumerated by $[n] := \{1,2,\ldots,n\}$.
Each arm $i \in [n]$ is associated with a reward, which is a binary random variable with expectation $p_i \in [0,1]$.
For convenience, we assume that the arms are ordered by their expected rewards, that is $p_1 \ge p_2 \ge \cdots \ge p_n$.
At every time step $t=1,2,\ldots,T$, each player pulls one arm of his choice and observes an independent sample of its reward. 
%\rl{This is actually too strong a constraint. If you force all players to pull simultaneously per step, then if $k$ is large enough you cannot simulate the single player even with communication rounds after each time step, since the single player pulls one arm and reevaluates while the above formulation would mean to pull k arms and reevaluate. A delicate change is needed} \tk{I added the word ``may'' - does it work?}
Each player may choose any of the arms, regardless of the other players and their actions.
At the end of the game, each player must commit to a single arm. 
In a \emph{communication round}, that may take place at any predefined time step, each player may broadcast a message to all other players.
%In our model, a round may take place at any predefined time step and is assumed not to cause any delay to the players, nor consume extra time steps.
While we do not restrict the size of each message, in a reasonable implementation a message should not be larger than $\tO(n)$ bits.

In the \emph{best-arm identification} version of the problem, the goal of a multi-player algorithm given some target confidence level $\del>0$, is that with probability at least $1-\del$ \emph{all} players correctly identify the best arm (i.e.~the arm having the maximal expected reward).
For simplicity, we assume in this setting that the best arm is unique.
Similarly, in the $(\eps,\del)$-PAC variant the goal is that each player finds an $\eps$-optimal (or ``$\eps$-best'') arm, that is an arm $i$ with $p_i \ge p_1 - \eps$, with high probability.
In this paper we focus on the more general $(\eps,\del)$-PAC setup, which also includes best-arm identification for $\eps=0$. 
%We use the term \emph{sample complexity} to refer to the number of arm pulls \emph{per player} needed for achieving the players' goal (at the desired confidence).

We use the notation $\Del_i := p_1 - p_i$ to denote the suboptimality gap of arm $i$, and occasionally use~$\Delst := \Del_2$ for denoting the minimal gap. In the best-arm version of the problem, where we assume that the best arm is unique, we have $\Del_i > 0$ for all $i$.
When dealing with the $(\eps,\del)$-PAC setup, we also consider the truncated gaps $\Dele_i := \max\{\Del_i,\eps\}$.
In the context of MAB problems, we are interested in deriving distribution-dependent bounds, namely, bounds that are stated as a function of~$\eps,\del$ and also the distribution-specific values  $\Del := (\Del_2,\ldots,\Del_n)$.
%%Note, however, that our algorithms do \emph{not} require any distribution-specific parameters.
The $\tO$ notation in our bounds hides polylogarithmic factors in $n,k,\eps,\del$, and also in~$\Del_2,\ldots,\Del_n$.
In the case of serial exploration algorithms (i.e., when there is only one player), the lower bounds of \citet{mannor04} and \citet{audibert10} show that in general $\tOm(H_\eps)$ pulls are necessary for identifying an $\eps$-arm, where
\begin{align} \label{eq:H}
	H_\eps :=
	\sum_{i=2}^{n} \frac{1}{(\Dele_i)^2} \,.
\end{align}
Intuitively, the hardness of the task is therefore captured by the quantity $H_\eps$, which is roughly the number of arm pulls needed to find an $\eps$-best arm with a reasonable probability; see also \cite{audibert10} for a discussion.
Our goal in this work is therefore to establish bounds in the distributed model that are expressed as a function of $H_\eps$, in the same vein of the bounds known in the classic MAB setup%
\footnote{If one is interested in distribution-free bounds, then the problem at hand is trivial as the (distributed) uniform sampling strategy is optimal in this setting, up to polylogarithmic factors; see also \cite{mannor04} for a relevant discussion.}. 






\subsection{Baseline approaches}

We now discuss several baseline approaches for the problem, starting with our main focus---the single round setting.
The first obvious approach, already mentioned earlier, is the \emph{no-communication} strategy: just let each player explore the arms in isolation of the other players, following an independent instance of a some serial strategy; at the end of the executions, all players hold an $\eps$-best arm.
Clearly, this approach performs poorly in terms of learning performance, needing $\tOm(H_\eps)$ pulls per player in the worst case and not leading to any parallel speed-up.

Another straightforward approach is to employ a \emph{majority vote} among the players: let each player independently identify an arm, and choose the arm having most of the votes (alternatively, at least half of the votes).
However, this approach does not lead to any improvement in performance: for this vote to work, each player has to solve the problem correctly with reasonable probability, which already require $\tOm(H_\eps)$ pulls of each. 
Even if we somehow split the arms between players and let each player explore a share of them, a majority vote would still fail since those players getting the ``good'' arms might have to pull arms $\tOm(H_\eps)$ times---a small MAB instance might be as hard as the full-sized problem (in terms of the complexity measure $H_\eps$).

When considering algorithms employing multiple communication rounds, we use an ideal \emph{simulated serial} algorithm (i.e., a full-communication approach) as our baseline. 
This approach is of course prohibited in our context, but is able to achieve the optimal parallel speed-up, linear in the number of players $k$.



%\paragraph{Our results.}
\subsection{Our results}

We now discuss our approach and overview our algorithmic results.  
These are summarized in Table~\ref{tab:results} below, that compares the different algorithms in terms of parallel speed-up and communication.

Our approach for the one-round case is based on the idea of majority vote. 
For the best-arm identification task, our observation is that by letting each player explore a smaller set of~$n/\sqrt{k}$ arms chosen at random and choose one of them as ``best'', about $\sqrt{k}$ of the players would come up with the \emph{global} best arm.
This (partial) consensus on a single arm is a key aspect in our approach, since it allows the players to  identify the correct best arm among the votes of all $k$ players, after sharing information only once.
Our approach leads to a factor $\sqrt{k}$ parallel speed-up which, as we demonstrate in our lower bound, is the optimal factor in this setting.
Although our goal here is pure exploration, in our algorithms each player follows an explore-exploit strategy. 
The idea is that a player should sample his recommended arm as much as his budget permits, even if it was easy to identify in his small-sized problem. % (i.e., did not require many pulls).
This way we can guarantee that the top arms are sampled to a sufficient precision by the time each of the players has to choose a single best arm.

The algorithm for the $(\eps,\del)$-PAC setup is similar, but its analysis is more challenging.
As mentioned above, an agreement on a single arm is essential for a vote to work. 
Here, however, there might be several $\eps$-best arms, so arriving at a consensus on a single one is more difficult.
Nonetheless, by examining two different regimes, namely when there are ``many'' $\eps$-best arms and when there are ``few'' of them, our analysis shows that a vote can still work and achieve the $\sqrt{k}$ multiplicative speed-up.
%This is detailed in Section~\ref{sec:oneround_eps}.

In the case of multiple communication rounds, we present a distributed elimination-based algorithm that discards arms right after each communication round. 
Between rounds, we share the work load between players uniformly. 
We show that the number of such rounds can be reduced to as low as $\O(\log(1/\eps))$, by eliminating all $2^{-r}$-suboptimal arms in the $r$'th round.
A similar idea was employed in \cite{auer2010ucb} for improving the regret bound of UCB with respect to the parameters $\Del_i$.
We also use this technique to develop an algorithm that performs only $r$ communication rounds, for any given parameter $r\ge1$, that achieves a slightly worse multiplicative $\eps^{2/r} k$ speed-up.
%As a corollary, A rather simple modification of this result yields an algorithm that uses at most $r$ rounds and $\tO(\eps^{-2/r} \, H_\eps)$ arm pulls overall, for any positive integer $r$.



% cellspace defs
\setlength{\cellspacetoplimit}{2pt}
\setlength{\cellspacebottomlimit}{2pt}

\begin{table}[htdp]
\begin{center}
\begin{tabular}{l||l|Sc|Sl}
\hline
\textsc{Setting} & \textsc{Algorithm} & \textsc{Speed-up} & \textsc{Communication}  \\
\hline\hline
\multirow{2}{*}{\textsc{One-Round}} 
	&No-Communication & $1$ & none \\
	&Majority Vote & $1$ & 1 round \\
	&Algorithm~\ref{alg:oneround},\ref{alg:oneround_eps} & $\sqrt{k} $ & $1$ round \\
\hline
\multirow{3}{*}{\textsc{Multi-Round}} 
	& Serial (simulated) & $k$ & every time step \\
	& Algorithm~\ref{alg:algname} & $k$ & $\O(\log \tfrac{1}{\eps})$ rounds \\
	& Algorithm~\ref{alg:algname}' & $\eps^{2/r} \cdot k$ & $r$ rounds \\
\hline
\end{tabular}
\end{center}
\caption{Summary of baseline approaches and our results. The speed-up results are asymptotic (logarithmic factors are omitted).} \label{tab:results}
\end{table}






%\input{singleround}
\section{One Communication Round}
\label{sec:singleround}
%\vspace{-1ex}



This section considers the most basic variant of the multi-player MAB problem, where each player is only allowed a single transmission, when finishing her queries.
%Here, given a budget of arm pulls, each player decides which arms to pull according to a policy based solely on his own observation.
%After exhausting the given budget, the players share their results and compute the output.
For the clarity of exposition, we first consider the best-arm identification setting in Section~\ref{sec:best one round}.
Section~\ref{sec:oneround_eps} deals with the $(\eps,\del)$-PAC setup.
We demonstrate the tightness of our result in Section~\ref{sec:lower bound} with a lower bound for the required budget of arm pulls in this setting.  

Our algorithms in this section assume the availability of a serial algorithm $\mathcal{A}(A, \eps)$, that given a set of arms~$A$ and target accuracy~$\eps$, identifies an $\eps$-best arm in $A$ with probability at least $2/3$ using no more than
\begin{align} \label{eq:se-bound}
%	\OO{
		\cA \sum_{i \in A} \frac{1}{(\Dele_i)^2} \log \frac{|A|}{\Dele_i}
%	}
\end{align}
arm pulls, for some constant $\cA > 1$.
For example, the Successive Elimination algorithm~\cite{evendar06}~provides a guarantee of this form.
%\tk{more examples?} \zk{yes - I heard of a recent exiting result in icml obtaining something even better...} \tk{not sure we need them, SE is already well-hidden...}
Essentially, any exploration strategy whose guarantee is expressed as a function of $H_\eps$ can be used as the procedure $\mathcal{A}$, with technical modifications in our analysis.
%and the choice of Successive Elimination here is rather arbitrary..


%\vspace{-1ex}
\subsection{Best-arm Identification Algorithm} \label{sec:best one round}
%\vspace{-1ex}

We now describe our one-round best-arm identification algorithm. 
For simplicity, we present a version matching $\delta=1/3$, meaning that the algorithm produces the correct arm with probability at least $2/3$; we later explain how to extend it to deal with arbitrary values of $\delta$. 

Our algorithm is akin to a majority vote among the multiple players, in which each player pulls arms in two stages. 
In the first \textsc{Explore} stage, each player independently solves a ``smaller'' MAB instance on a random subset of the arms using the exploration strategy $\mathcal{A}$.
In the second \textsc{Exploit} stage, each player exploits the arm identified as ``best'' in the first stage, and communicates that arm and its observed average reward. 
See Algorithm \ref{alg:oneround} below for a precise description.
An appealing feature of our algorithm is that it requires each player to transmit a single message of constant size~(up to logarithmic factors).
% communicate merely two numerical values to the other players during the communication round. \zk{transmission size is constant}



\begin{wrapfigure}[21]{R}{0.5\textwidth}\begin{minipage}{0.5\textwidth}
\vspace{-22pt}

\begin{algorithm}[H] \caption{\textsc{One-round Best-arm}} \label{alg:oneround}
\begin{algorithmic}[1]

\INPUT{time horizon $T$}
\OUTPUT {an arm}

\FOR {player $j=1$ to $k$}
	\STATE choose a subset $A_j$ of $6 n / \sqrt{k}$ arms uniformly at random
	\STATE \textsc{Explore}: execute $i_j \gets \mathcal{A}(A_j,0)$ using at most $\tfrac{1}{2} T$ pulls (and halting the algorithm early if necessary); 
	
	if the algorithm fails to identify any arm or does not terminate gracefully, let $i_j$ be an arbitrary arm
	\STATE \textsc{Exploit}: pull arm $i_j$ for $\tfrac{1}{2} T$ times and let~$\qhat_j$ be its average reward
	\STATE communicate the numbers $i_j, \qhat_j$
\ENDFOR

\STATE let $k_i$ be the number of players $j$ with $i_j = i$, and 
define $A = \{i \,:\, k_i > \sqrt{k} \}$

\STATE let $\phat_i = (1/k_i) \sum_{\{j \,:\, i_j = i\}} \qhat_j$ for all $i$

\STATE {\bf return} $\arg \max_{i \in A} \phat_i$; if the set $A$ is empty, output an arbitrary arm. 

\end{algorithmic}
\end{algorithm}

\end{minipage}\end{wrapfigure}




In Theorem \ref{thm:oneround} we prove that Algorithm \ref{alg:oneround} indeed
achieves the promised upper bound.

\begin{theorem} \label{thm:oneround}
Algorithm \ref{alg:oneround} identifies the best arm correctly with probability at least $2/3$ using no more than
\begin{align*}% \label{eq:oneround_bound}
	\OO {
		\frac{1}{\sqrt{k}} \cdot
		\sum_{i=2}^{n} \frac{1}{\Del_i^2} \log \frac{n}{\Del_i} 
	} 
\end{align*}
%$
%	\tO\left( \tfrac{1}{\sqrt{k}} \sum_{i=2}^{n} 1/\Del_i^2 \right)
%$
arm pulls per player, provided that $6 \le \sqrt{k} \le n$.
The algorithm uses a single communication round, in which each player communicates $\tO(1)$ bits.
\end{theorem}


By repeating the algorithm $\O(\log(1/\del))$ times and taking the majority vote of the independent runs, we can amplify the success probability to $1-\del$ for any given $\del > 0$. 
Note that we can still do that with one communication round (at the end of all executions), but each player now has to communicate $\O(\log(1/\del))$ values%
\footnote{In fact, by letting each player pick a slightly larger subset of $\O(\sqrt{\log (1/\del)} \cdot n/\sqrt{k})$ arms, we can amplify the success probability to $1-\del$ without needing to communicate more than 2 values per player. However, this approach only works when $k = \Omega(\log(1/\del))$.}. 


\begin{theorem}
There exists a $k$-player algorithm that given
%\begin{align*}
%	\OO {
%		\frac{1}{\sqrt{k}} \cdot
%		\sum_{i=2}^{n} \frac{1}{\Del_i^2} \log \frac{n}{\Del_i} \log \frac{1}{\del}
%	}
%\end{align*}
$
	\tO\left( \tfrac{1}{\sqrt{k}} \sum_{i=2}^{n} 1/\Del_i^2 \right)
$
arm pulls, identifies the best arm correctly with probability at least $1-\del$.
The algorithm uses a single communication round, in which each player communicates $O(\log(1/\delta))$ numerical values.
\end{theorem}



%\subsubsection{Analysis}

We now prove Theorem~\ref{thm:oneround}.
We show that a budget $T$ of samples (arm pulls) per player, where%
%\footnote{Here and elsewhere, $\log$ refers to the natural logarithm.}
\begin{align} \label{eq:T}
	T 
	\ge \frac{24 \cA}{\sqrt{k}} \cdot
			\sum_{i=2}^{n} \frac{1}{\Del_i^2} \ln \frac{n}{\Del_i} \,,
\end{align}
suffices for the players to jointly identify the best arm $i^\st$ with the desired probability.
Clearly, this would imply the bound stated in Theorem~\ref{thm:oneround}. 
We note that we did not try to optimize the constants in the above expression. 


We begin by analyzing the \textsc{Explore} phase of the algorithm. Our first lemma shows that each player chooses the global best arm and identifies it as the local best arm with sufficiently large probability.


\begin{lemma} \label{lem:kexplore}
When \eqref{eq:T} holds, each player identifies the (global) best arm correctly after the \emph{\textsc{Explore}} phase with probability at least $2/\sqrt{k}$.
\end{lemma}

We next address the \textsc{Exploit} phase. The next simple lemma shows that the
popular arms (i.e.~those selected by many players) are estimated to a sufficient
precision.  


\begin{lemma} \label{lem:kexploit}
Provided that \eqref{eq:T} holds, we have $\abs{\phat_i - p_i} \le \tfrac{1}{2} \Delst$ for all arms $i \in A$ with probability at least $5/6$. 
\end{lemma}



Due to space constraints, the proofs of the above lemmas are omitted and can be found in the supplementary material.
We can now prove Theorem~\ref{thm:oneround}.

\begin{proof}[Proof (of Theorem~\ref{thm:oneround})]
Let us first show that with probability at least $5/6$, the best arm $i$ is contained in the set $A$.
To this end, notice that $k_{i^\st}$ is the sum of $k$ i.i.d.~Bernoulli random variables $\{I_j\}_{j}$ where $I_j$ is the indicator of whether player $j$ chooses arm $i^\st$ after the \textsc{Explore} phase. By Lemma~\ref{lem:kexplore} we have that $\E[I_j] \geq 2/\sqrt{k}$ for all $j$, hence by Hoeffding's inequality,
$
	\Pr[ k_{i^\st} \le \sqrt{k} ] 
%	\le \Pr\left[ \frac{1}{k}\sum\nolimits_{j=1}^k (I_j - \E[I_j]) \le -\frac{1}{\sqrt{k}} \right]
	\le \Pr[ k_{i^\st} - \E[k_{i^\st}] \le -\sqrt{k} ]
	\le \exp(-2k/k) 
	\le 1/6
$
which implies that $i^\st \in A$ with probability at least $5/6$.


Next, note that with probability at least $5/6$ the arm $i \in A$ having the highest empirical reward $\phat_i$ is the one with the highest expected reward $p_i$.
Indeed, this follows directly from Lemma \ref{lem:kexploit} that shows that with probability at least $5/6$, for all arms $i \in A$ the estimate $\phat_i$ is within $\frac{1}{2}\Del$ of the true bias $p_i$.
Hence, via a union bound we conclude that with probability at least $2/3$, the best arm is in $A$ and has the highest empirical reward. 
In other words, with probability at least $2/3$ the algorithm outputs the best arm~$i^\st$.
\end{proof}








\subsection{$(\eps,\del)$-PAC Algorithm} \label{sec:oneround_eps}
%\vspace{-1ex}

We now present an algorithm whose purpose is to recover an $\eps$-optimal arm.
Here, there might be more than one $\eps$-best arm, so each ``successful'' player might come up with a different $\eps$-best arm.
%This makes this setting more challenging since arriving at a consensus on a single arm, though not a requirement of the model, is crucial for the analysis of the distributed strategy. 
Nevertheless, our analysis below shows that with high probability, a subset of the players can still agree on a single $\eps$-best arm, which makes it possible to identify it among the votes of all players.
Our algorithm is described in Algorithm~\ref{alg:oneround_eps}, and the following theorem states its guarantees.



\begin{wrapfigure}[20]{R}{0.5\textwidth}\begin{minipage}{0.5\textwidth}
\vspace{-10pt}

\begin{algorithm}[H] \caption{\textsc{One-round $\eps$-arm}} \label{alg:oneround_eps}
\begin{algorithmic}[1]

\INPUT{time horizon $T$, accuracy $\eps$}
\OUTPUT {an arm}

\FOR {player $j=1$ to $k$}
	\STATE choose a subset $A_j$ of $12n/\sqrt{k}$ arms uniformly at random
	\STATE \textsc{Explore}: execute $i_j \gets \mathcal{A}(A_j,\eps)$ using at most $\tfrac{1}{2}T$ pulls (and halting the algorithm early if necessary); 
	
	if the algorithm fails to identify any arm or does not terminate gracefully, let $i_j$ be an arbitrary arm
	\STATE \textsc{Exploit}: pull arm $i_j$ for $\tfrac{1}{2}T$ times, and let~$\qhat_j$ be the average reward
	\STATE communicate the numbers $i_j, \qhat_j$
\ENDFOR

\STATE let $k_i$ be the number of players $j$ with $i_j = i$

\STATE let $t_i = \tfrac{1}{2} k_i T$ and 
$\phat_i = (1/k_i) \sum_{\{j \,:\, i_j = i\}} \qhat_j$ for all $i$

\STATE define $A = \{i \in [n] \,:\, t_i \ge (4/\eps^2) \ln (12n)\}$

\STATE {\bf return} $\arg \max_{i \in A} \phat_i$; if the set $A$ is empty, output an arbitrary arm. 
\end{algorithmic}
\end{algorithm}

\end{minipage}\end{wrapfigure}




\begin{theorem} \label{thm:oneround_eps}
Algorithm \ref{alg:oneround_eps} identifies a $2\eps$-best arm with probability at least $2/3$ using no more than
\begin{align*}% \label{eq:1rbound_eps}
	\OO {
		\frac{1}{\sqrt{k}} \cdot
		\sum_{i=2}^{n} \frac{1}{(\Del_i^\eps)^2} \log \frac{n}{\Del_i^\eps}
	}
\end{align*}
%$
%	\tO\left( \tfrac{1}{\sqrt{k}} \sum_{i=2}^{n} (1/\Del_i^\eps)^2 \right)
%$
arm pulls per player, provided that $24 \le \sqrt{k} \le n$.
The algorithm uses a single communication round, in which each player communicates $\tO(1)$ bits.
\end{theorem}




%\subsubsection{Analysis}

Before proving the theorem, we first state several key lemmas. In the following,
let $n_\eps$ and $n_{2\eps}$ denote the number of $\eps$-best and $2\eps$-best arms respectively.
Our analysis considers two different regimes: $n_{2\eps} \le \tfrac{1}{50}\sqrt{k}$ and $n_{2\eps} > \tfrac{1}{50}\sqrt{k}$, and shows that in any case,
\begin{align} \label{eq:T_eps}
	T \ge
	\frac{400 \cA}{\sqrt{k}} 
		\sum_{i=2}^{n} \frac{1}{(\Del_i^{\eps})^2}\ln \frac{24n}{\Del_i^{\eps}}
\end{align}
suffices for identifying a $2\eps$-best arm with the desired probability.
Clearly, this implies the bound stated in Theorem~\ref{thm:oneround_eps}.

The first lemma shows that at least one of the players is able to find an $\eps$-best arm. As we later show, this is sufficient for the success of the algorithm in case there are many $2\eps$-best arms.



\begin{lemma} \label{lem:kexplore_eps2}
When \eqref{eq:T_eps} holds, at least one player successfully identifies an $\eps$-best arm in the \emph{\textsc{Explore}} phase, with probability at least~$5/6$.
\end{lemma}


The next lemma is more refined and states that in case there are few $2\eps$-best arms, the probability of each player to successfully identify an $\eps$-best arm grows linearly with $n_\eps$.


\begin{lemma} \label{lem:kexplore_eps1}
Assume that $n_{2\eps} \le \tfrac{1}{50}\sqrt{k}$. When \eqref{eq:T_eps} holds, each player identifies an $\eps$-best arm in the \emph{\textsc{Explore}} phase, with probability at least $2 n_\eps / \sqrt{k}$.
\end{lemma}



The last lemma we need analyzes the accuracy of the estimated rewards of arms in
the set $A$. %\eh{???}

\begin{lemma} \label{lem:estimates_eps}
With probability at least $5/6$, we have $\abs{\hat{p}_i - p_i} \le \eps/2$ for all arms $i \in A$.
\end{lemma}



For the proofs of the above lemmas, refer to the supplementary material.
We now turn to prove Theorem~\ref{thm:oneround_eps}.


\begin{proof}
We shall prove that with probability $5/6$ the set $A$ contains at least one $\eps$-best arm.
This would complete the proof, since Lemma~\ref{lem:estimates_eps} assures that with probability $5/6$, the estimates $\phat_i$ of all arms $i \in A$ are at most $\eps/2$-away from the true reward $p_i$, and in turn implies (via a union bound) that with probability $2/3$ the arm $i \in A$ having the maximal empirical reward $\phat_i$ must be a $2\eps$-best arm.

First, consider the case $n_{2\eps} > \tfrac{1}{50}\sqrt{k}$.
Lemma \ref{lem:kexplore_eps2} shows that with probability $5/6$ there exists a player $j$ that identifies an $\eps$-best arm $i_j$.
Since for at least $n_{2\eps}$ arms $\Del_i \le 2\eps$, we have
\begin{align*}
	t_{i_j} 
	&\ge \tfrac{1}{2} T
	\ge \frac{400}{2\sqrt{k}} \cdot \frac{n_{2\eps}-1}{(2\eps)^2} \ln \frac {24n}{2\eps} \\
	&\ge \frac{1}{\eps^2} \ln (12n) \,,
\end{align*}
that is, $i_j \in A$.

Next, consider the case $n_{2\eps} \le \tfrac{1}{50}\sqrt{k}$.
Let $N$ denote the number of players that identified some $\eps$-best arm.
The random variable~$N$ is a sum of Bernoulli random variables $\{I_j\}_j$ where $I_j$ indicates whether player $j$ identified some $\eps$-best arm. By Lemma~\ref{lem:kexplore_eps1}, $\E[I_j]\geq 2 n_{\eps} / \sqrt{k}$ and thus by Hoeffding's inequality,
$
	\Pr[ N < n_{\eps} \sqrt{k} ] 
%	= \Pr\left[ \frac{1}{k} \sum\nolimits_{j=1}^k (I_j-\E[I_j]) \le -\frac{n_\eps}{\sqrt{k}} \right]
	= \Pr[ N-\E[N] \le - n_\eps \sqrt{k} ]
	\le \exp(-2 n_{\eps}^2)
	\le 1/6 \,.
$
That is, with probability $5/6$, at least $n_{\eps} \sqrt{k}$ players found an $\eps$-best arm. 
A~pigeon-hole argument now shows that in this case there exists an $\eps$-best arm $i^\st$ selected by at least $\sqrt{k}$ players.
Hence, with probability $5/6$ the number of samples of this arm collected in the \textsc{Exploit} phase is at least $t_{i^\st} \ge \sqrt{k} T/2 > (1/\eps^2) \ln(12n)$, which means that $i^\st \in A$.
\end{proof}





\subsection{Lower Bound} \label{sec:lower bound}
%\vspace{-1ex}


The following theorem suggests that in general, for identifying the best arm $k$ players achieve a multiplicative speed-up of at most  $\tilde{O}(\sqrt{k})$ when allowing one transmission per player (at the end of the game). 
Clearly, this also implies that a similar lower bound holds in the PAC setup, and proves that our algorithmic results for the one-round case are essentially tight.    


\begin{theorem} \label{thm:lb1}
There exist rewards $p_1,\ldots,p_n \in [0,1]$ and integer $T$ such that
\begin{itemize}
\item
each player in the algorithm must use at least $T/\sqrt{k}$ arm pulls for
them to collectively identify the best arm with probability at least $2/3$;
\item
there exist a single-player algorithm that needs at most $\tO(T)$ pulls for
identifying the best arm with probability at least $2/3$.
\end{itemize}
\end{theorem}


The proof of the theorem is deferred to the supplementary material.




%\input{multiplerounds}
%\vspace{-1ex}
\section{Multiple Communication Rounds}
\label{sec:multiplerounds}
%\vspace{-1ex}


In this section we establish an explicit tradeoff between the performance of a multi-player
algorithm and the number of communication rounds it uses, in terms of the accuracy~$\eps$.
Our observation is that by allowing $\O(\log(1/\eps))$ rounds of communication, it is possible to achieve an optimal speedup of factor $k$. That is, we do not gain any improvement in learning performance by allowing more than $\O(\log(1/\eps))$ rounds.


\begin{wrapfigure}[19]{R}{0.5\textwidth}\begin{minipage}{0.5\textwidth}
\vspace{-12pt}

\begin{algorithm}[H] \caption{\textsc{Multi-Round $\eps$-Arm}} \label{alg:algname}
\begin{algorithmic}[1]

\INPUT{$(\eps,\del)$}
\OUTPUT{an arm}

\STATE initialize $S_0 \gets [n]$, $r \gets 0, t_0 \gets 0$

\REPEAT
	\STATE set $r \gets r+1$
	\STATE let $\eps_r \gets 2^{-r}, t_r \gets (2/k\eps_r^2) \ln (4 n r^2/\del)$
	\FOR {player $j=1$ to $k$}
		\STATE sample each arm $i \in S_{r-1}$  for $t_r - t_{r-1}$ times
		\STATE let $\phat_{j,i}^r$ be the average reward of arm $i$ (in all rounds
		so far of player $j$) 
		\STATE communicate the numbers $\phat_{j,1}^r, \ldots,
		\phat_{j,n}^r$
	\ENDFOR
	
	\STATE let $\phat_i^r = (1/k) \sum_{j=1}^k \phat_{j,i}^r$ for all $i \in S_{r-1}$,
		and let $\phat_{\st}^r = \max_{i \in S_{r-1}} \phat_i^r$
	\STATE set 
		$
		S_r \gets S_{r-1} \setminus 
		\{i \in S_{r-1} : \phat_i^r < \phat_{\st}^r - \eps_r\}
		$
\UNTIL {$\eps_r \le \eps/2$ or $\abs{S_r} = 1$}

\STATE {\bf return} an arm from $S_r$

\end{algorithmic}
\end{algorithm}

\end{minipage}\end{wrapfigure}



Our algorithm is given in Algorithm~\ref{alg:algname}.
The idea is to eliminate in each round $r$ (i.e., right after the $r$th communication round) all $2^{-r}$-best arms.
We accomplish that by letting each player sample uniformly all remaining arms and communicate the results to other players. Then, players are able to eliminate suboptimal arms with high confidence.
If each such round is successful, after $\log_2(1/\eps)$ rounds only $\eps$-best arms survive.
Theorem \ref{thm:main1} below bounds the number of arm pulls used by this algorithm (proof is deferred to the supplementary material).



\begin{theorem} \label{thm:main1}

With probability at least $1-\del$, Algorithm \ref{alg:algname}
\begin{itemize}
\item
identifies the optimal arm using
\begin{align*}
	\OO{
		\frac{1}{k} \cdot \sum_{i=2}^{n}
		\frac{1}{(\Dele_i)^2} \log \left(\frac{n}{\del} \log \frac{1}{\Dele_i}\right)
	}
\end{align*}
%$
%	\tO (\tfrac{1}{k} \sum_{i=2}^{n} (1/\Dele_i)^2)
%$
arm pulls per~player;
\end{itemize}

\begin{itemize} % break itemize for better layout in NIPS format
\item
terminates after at most $1+\ceil{\log_2(1/\eps)}$ rounds of communication (or after $1+\ceil{\log_2(1/\Del)}$ rounds for $\eps=0$).
\end{itemize}

\end{theorem}






%\subsection{$R$-rounds Algorithm}
%\vspace{-1ex}


By properly tuning the elimination thresholds $\eps_r$ of Algorithm~\ref{alg:algname} in accordance with the target accuracy $\eps$, we can establish an explicit trade-off between the number of communication rounds and the number of arm pulls each player needs.
In particular, we can design a multi-player algorithm that terminates after at
most~$R$ communication rounds, for any given parameter $R>0$.
This, however, comes at the cost of a compromise in learning performance as quantified in the following corollary.


\begin{corollary} \label{cor:main3}
Given a parameter $R > 0$, set $\eps_r \gets \eps^{r/R}$ for all $r \ge 1$ in Algorithm \ref{alg:algname}.
With~probability at least $1-\del$, the modified algorithm%
%\footnote{In contrast to other algorithms discussed in this paper, this algorithm does not apply to $\eps=0$. In order to apply it in a best-arm identification setting, one should set $0<\eps\le\Del$, but this would require knowing the value of $\Del$ (or some lower bound thereof) beforehand. }  
\begin{itemize}
\item
identifies an $\eps$-best arm using
%\begin{align*}
%	\OO{
%		\frac{1}{\eps^{2/R}}
%		\cdot
%		\sum_{i=2}^{n} \frac{1}{(\Dele_i)^2}
%		\log \frac{n R}{\del}
%	}
%\end{align*}
\begin{align*}
	\tO\lr{
		\frac{\eps^{-2/R}}{k}
		\cdot
		\sum_{i=2}^{n} \frac{1}{(\Dele_i)^2}
	}
\end{align*}
%$
%	\tO( (\eps^{-2/R}/k) \cdot \sum_{i=2}^{n} (1/\Dele_i)^2 )
%$
arm pulls per player;
\item
terminates after at most $R$ rounds of communication.
\end{itemize}
\end{corollary}



%\input{conc.tex}

\section{Conclusions and Further Research} \label{sec:conc}
%\vspace{-1ex}

We have considered a collaborative MAB exploration problem, in which several independent players explore a set of arms with a common goal, and obtained the first non-trivial results in such setting.
Our main results apply for the specifically interesting regime where each of the players is allowed a single transmission; this setting 
%is natural for large-scale problems where the bottleneck (in terms of computation time) resides in the evaluation of rewards and 
fits naturally to common distributed frameworks such as MapReduce. 
An interesting open question in this context is whether one can obtain a strictly better speed-up result (which, in particular, is independent of $\eps$) by allowing more than a single round.
Even when allowing merely two communication rounds, it is unclear whether the $\sqrt{k}$ speed-up can be improved. 
Intuitively, the difficulty here is that in the second phase of a reasonable strategy each player should focus on the arms that excelled in the first phase; this makes the sub-problems being faced in the second phase as hard as the entire MAB instance, in terms of the quantity $H_\eps$.
Nevertheless, we expect our one-round approach to serve as a building-block in the design of future distributed exploration algorithms, that are applicable in more complex communication models.
%future, more complex distributed exploration algorithms.

An additional interesting problem for future research is how to translate our results to the regret minimization setting.
In particular, it would be nice to see a conversion of algorithms like UCB~\cite{auer2002finite} to a distributed setting.
In this respect, perhaps a more natural distributed model is a one more resembling that of \citet{kanade2012distributed}, that have established a regret vs.~communication trade-off in the non-stochastic setting.





%\bibliographystyle{plainnat}
\bibliographystyle{abbrvnat}
\bibliography{dmab}


% begin conf. omission
\ifdefined\isconf
\else

\newpage
\pagenumbering{gobble} % suppress page numbers (for supplementary material)
\appendix

\begin{center}
\section*{}
{\Large\bf Distributed Exploration in Multi-Armed Bandits:} \\
{\large\bf Supplementary Material}
\end{center}


\section{Proofs of Lemmas in Section \ref{sec:singleround}}

For the proofs in this section, we need some additional notation.
For any player $j$, let $i^\st_j$ denote the best arm in $A_j$, with ties broken arbitrarily. 
Let $\Del^\st_j := \Del_{i^\st_j} = \min_{i \in A_j} \Del_i$ and $\Del_{i,j} := \max\{ \Del_i - \Del_j^\st, \eps \}$ for all $i \in A_j$.
Finally, define
\begin{align*}
	H
	:= \sum_{i^\st \ne i \in [n]}
		\frac{1}{(\Del_i^{2\eps})^2} \ln \frac{2n}{\Del_i^{2\eps}}
\end{align*}
and
\begin{align*}
	H_j^{\text{local}}
	:= \sum_{i^\st_j \ne i \in A_j}
		\frac{1}{\Del_{i,j}^2} \ln \frac{n}{\Del_{i,j}}
	\;,\qquad
	H_j^{\text{global}}
	:= \sum_{i^\st_j \ne i \in A_j}
		\frac{1}{(\Del_i^{2\eps})^2} \ln \frac{2n}{\Del_i^{2\eps}}
\end{align*}
for all players $j$.





\begin{proof}[Proof (of Lemma~\ref{lem:kexplore})]
Let $H = \sum_{i \ne i^\st} (1/\Del_i^2) \ln (n/\Del_i)$ and
$H_j = \sum_{i^\st \ne i \in A_j} (1/\Del_i^2) \ln (n/\Del_i)$ for all $j$.
Then $\E[H_j \mid i^\st \in A_j] \le 6H/\sqrt{k}$ by the linearity of expectation, and Markov's inequality thus gives that
$
	\Pr[H_j \le 12H/\sqrt{k} \mid i^\st \in A_j]
	\ge 1/2 .
$
Clearly, we also have $\Pr[i^\st \in A_j] = 6/\sqrt{k}$ which implies that
\begin{align} \label{eq:H_j}
	\Pr[i^\st \in A_j \mbox{ and } H_j \le 12H/\sqrt{k}] \ge 3/\sqrt{k} \,.
\end{align}
Now consider the ``local'' MAB problem facing player $j$, over the subset of arms $A_j$. If the (global) best arm $i^\st$ is amongst the arms in $A_j$, then by eq.~\eqref{eq:se-bound} the instance of the procedure $\mathcal{A}$ player~$j$ executes needs no more than
\begin{align*}
	T_j
	:= \cA \sum_{i^\st \ne i \in A_j} \frac{1}{\Del_i^2}\ln \frac{n}{\Del_i}
	= \cA H_j
\end{align*}
pulls in order to identify $i^\st$ successfully with probability $2/3$. 
In case that $H_j \le 12H/\sqrt{k}$, we have
$
	T_j
	\le 12 \cA H / \sqrt{k}
	\le T/2 \,,
$
which means that the pulls budget of player $j$ suffices for identifying the
best arm. Together with \eqref{eq:H_j}, we conclude that with probability at
least $2/\sqrt{k}$ player~$j$ identifies the best arm correctly.
%\rl{so I'd replace $T/2$ by $3T/4$ in the above with the alternative definition of $T$.} 
\end{proof}




\begin{proof}[Proof (of Lemma~\ref{lem:kexploit})]
Consider some arm $i \in A$. The estimate $\phat_i$ is the average reward of 
$k_i T/2 \ge \sqrt{k} T/2 \ge (2/\Delst^2) \ln(12n)$
arm pulls (of the \textsc{Exploit} phase).
Hoeffding's inequality now gives that
\begin{align*}
	\Pr[\abs{\phat_i - p_i} > \tfrac{1}{2} \Delst]
	\le 2 \exp (-\tfrac{1}{2} \Delst^2 \cdot k_i T/2)
	\le 1/6n \,,
\end{align*}
and the lemma follows via a union bound.
\end{proof}




\begin{proof}[Proof (of Lemma~\ref{lem:kexplore_eps2})]
The proof is analogical to that of Lemma~\ref{lem:kexplore}.
Let $i^\st$ be the best arm and let $j$ be a player that chose it. The funds required by player $j$ in order to succeed choosing an $\eps$-best arm with probability at least $2/3$ is at most
$
	T_j	:= \cA \sum_{i^\st \ne i \in A_j}
		(1/\Del_i^{\eps})^2 \ln (n/\Del_i^{\eps})
$.
Given Eq.~\eqref{eq:T_eps}, $\E[T_j] \leq T/4$, meaning that by Markov $\Pr[T_j \le T/2] \geq 1/2$. Since the probability of choosing the best arm is $12/\sqrt{k}$ it follows that for any fixed player $j$, with probability at least $\tfrac{2}{3} \cdot \tfrac{1}{2} \cdot 12/\sqrt{k} = 4/\sqrt{k}$ the player identified an $\eps$-best arm.
Thus, the probability that all of the players fail to identify an $\eps$-best arm is bounded by
\begin{align*}
	\left( 1- \tfrac{4}{\sqrt{k}} \right)^k
	\le e^{-4\sqrt{k}}
	< 1/6 \,,
\end{align*}
and the lemma follows.
\end{proof}



\begin{proof}[Proof (of Lemma~\ref{lem:kexplore_eps1})]
For convenience, let $\alpha = 12/\sqrt{k}$ and note that by our assumptions $\alpha \le \tfrac{1}{2}$.
Also, since we assume $\sqrt{k} \le n$ we have $n_{2\eps} \le \tfrac{1}{2}n$.

Fix some player $j$ and let $X_{\eps}$ and $X_{2\eps}$ denote the number of $\eps$-best and $2\eps$-best arms chosen by this player, respectively. 
Consider the event $B = \{X_{\eps} = X_{2\eps} = 1\}$ in which the player chooses \emph{exactly one} $\eps$-optimal arm but no other $2\eps$-optimal arm.
The probability that this event occurs is
\begin{align*}
	\Pr[B]
	&= \frac{n_{\eps} {n - n_{2\eps} \choose \alpha n - 1}}
		{{n \choose \alpha n}}
	= \alpha n_{\eps} \frac{{n - n_{2\eps} \choose \alpha n - 1}}
		{{n-1 \choose \alpha n - 1}} \\
	&\ge \alpha n_{\eps} \left( \tfrac{n - n_{2\eps} - \alpha n}{n-\alpha n} \right)^{\alpha n}
	= \alpha n_{\eps} \left( 1 - \tfrac{n_{2\eps}}{n-\alpha n} \right)^{\alpha n} \\
	&\ge \alpha n_{\eps} \left( 1 - \tfrac{2 n_{2\eps}}{n} \right)^{\alpha n} %\\
%	\intertext{since $\alpha \le \tfrac{1}{2}$,}
		&&\mbox{(since $\alpha \le \tfrac{1}{2}$)} \\
	&\ge \alpha n_{\eps} \left( 1- 2 \alpha n_{2\eps} \right) %\\
%	\intertext{since $(1-x)^a \ge 1-ax$ for $x \le 1$, and}
		&&\mbox{($(1-x)^a \ge 1-ax$ for $x \le 1$)} \\
	&\ge \tfrac{1}{2}\alpha n_{\eps} = 6 n_{\eps}/\sqrt{k} %\\
%	\intertext{since $\alpha n_{2\eps} \le \frac{1}{4}$.}
		&&\mbox{(since $\alpha n_{2\eps} \le \frac{1}{4}$)}
\end{align*}

On the other hand, given the event $B$, for player $j$ we have $\Del_j^\st \le \eps$ and $\Del_i \ge 2\eps$ for all $i \ne i_j^\st$.
Consequently, $\Del_{i,j} \ge \tfrac{1}{2} \Del_i =  \tfrac{1}{2} \Del_i^{2\eps}$ for all $i \in A_j$.
Hence,
\begin{align*}
	H_j^\text{local}
	&= \sum_{i^\st_j \ne i \in A_j} \frac{1}{\Del_{i,j}^2} \ln \frac{n}{\Del_{i,j}} \\
	&\le 4 \sum_{i^\st_j \ne i \in A_j} \frac{1}{(\Del_i^{2\eps})^2} \ln \frac{2n}{\Del_i^{2\eps}} \\
	&= 4 \, H_j^\text{global}
\end{align*}
Denoting $\Del_{> 2\eps} := \{i \,:\, \Del_i > 2\eps\}$, we now have 
\begin{align*}
	\E[H_j^\text{local} \mid B]
	&\le 4 \, \E[H_j^\text{global} \mid B] \\
	&\le 4 \frac{12n}{\sqrt{k}} \cdot 
		\frac{1}{\abs{\Del_{>2\eps}}} 
			\sum_{i \in \Del_{>2\eps}} \frac{1}{\Del_i^2} \ln \frac{2n}{\Del_i} \\
	&< \frac{50n}{\sqrt{k}} \cdot 
		\frac{1}{n} \sum_{i \ne i^\st} \frac{1}{(\Del_i^{2\eps})^2} \ln \frac{2n}{\Del_i^{2\eps}} \\
	&= \frac{50}{\sqrt{k}} H
\end{align*}
Markov's inequality now gives
$
	\Pr[H_j^\text{local} \le 100 H/\sqrt{k} \mid B]
	\ge \tfrac{1}{2} ,
$
and together with $\Pr[B] \ge 6 n_{\eps} / \sqrt{k}$ we obtain that
\begin{align*}
	\Pr[B \mbox{ and } H_j^\text{local} \le 100 H/\sqrt{k} ]
	\ge 3 n_{\eps} / \sqrt{k} \,.
\end{align*}

Continuing as in the proof of Lemma \ref{lem:kexplore}, we get that when \eqref{eq:T_eps} holds, with probability at least $2 n_{\eps} / \sqrt{k}$
(i) $A_j$ contains an $\eps$-best arm which is the only $2\eps$-best arm in $A_j$, and
(ii) player~$j$ successfully identifies an arm which is $\eps$-best with respect to the best arm in~$A_j$. 
This implies that with probability at least $2 n_{\eps} / \sqrt{k}$, the arm $i_j$ selected by player $j$ is $\eps$-best.
\end{proof}



\begin{proof}[Proof (of Lemma~\ref{lem:estimates_eps})]
Since each estimate $\phat_i$ is the empirical average of at least $t_i$ samples of arm~$i$,
Hoeffding's inequality gives
\begin{align*}
	\Pr[\abs{\phat_i - p_i} > \eps/2]
	\le 2 \exp (-\eps^2 t_i/2)
	\le 1/6n \,,
\end{align*}
and a union bound concludes the proof.
\end{proof}



\section{Proof of Theorem~\ref{thm:lb1}}

Our proof for Theorem~\ref{thm:lb1} is based on a simple lower bound for the MAB problem, that follows directly from Lemma~5.1 of \cite{anthony1999neural}. 

\begin{lemma} \label{lem:coin}
Consider a MAB problem with two arms and rewards $\frac{1}{2} + \eps$, $\frac{1}{2} - \eps$.
There exists a constant $c$ such that any algorithm that with probability at least $2/3$ identifies the best arm, needs at least $c /\eps^2$ pulls in expectation.
\end{lemma}



\begin{proof}[Proof (of Theorem~\ref{thm:lb1})]
Let $c_1 = \sqrt{c}/2$, where $c$ is the constant of Lemma \ref{lem:coin}.
Consider a MAB instance over $n$ arms, with the rewards being a random
permutation $\sigma$ of $\frac{1}{2} + \Del, \frac{1}{2} - \Del, 0, \ldots, 0$, where $\Del := 1/\sqrt{n}$.
A serial algorithm of choice (say, the Successive Elimination algorithm) is able to identify the best arm in this setting with probability $2/3$ using at most $\tO(n + \Del^2) = \tO(n)$ arm pulls (see e.g., \cite{evendar06}).

Assume that the players follow some algorithm, each using no more than $c_1 n/\sqrt{k}$ pulls. Without loss of generality, we may assume that the sequence of rewards, as well as the internal random bits of the algorithm (if it is randomized), were drawn before the execution has started. We shall denote this sequence of random variables by $h$.

First, fix some arbitrary sequence $h$. Let $T_{i,j}$ denote the number of times arm $i$ (in decreasing order of rewards) was pulled by player $j$, and $T_i$ denote the total number of pulls of arm $i$.
Since the budget of each player is $c_1 n/\sqrt{k}$, we have $\Pr_\sigma[T_{1,j} > 0] \le c_1 / \sqrt{k}$ and consequently
\[
	\E_\sigma[T_{1,j}]
	\le \E_\sigma[T_{1,j} \mid T_{1,j} > 0] \cdot \Pr[T_{1,j} > 0]
	\le c_1^2 n/k
\]
for any player~$j$.
Similarly, $\E_\sigma[T_{2,j}] \le c_1^2 n/k$ and we get that 
\[
	\E_\sigma[T_1 + T_2] \le 2 c_1^2 n = 2 c_1^2 / \Del^2 < c / \Del^2 .
\]
Since the above holds for any given sequence $h$, this implies that $\E_\sigma[\E_h[T_1 + T_2]] < c / \Del^2$, which means that there exists a permutation $\sigma_0$ underwhich $\E_h[T_1 + T_2] < c / \Del^2$.
For the permutation~$\sigma_0$ and its corresponding reward setting, the algorithm does not sample the top two arms enough (in expectation), and by Lemma \ref{lem:coin} it cannot succeed with probability greater than $2/3$.
\end{proof}


\section{Proofs of Section~\ref{sec:multiplerounds}}



\begin{proof}[Proof (of Theorem~\ref{thm:main1})]
Without loss of generality, we may assume that the rewards of all arms are drawn before the algorithm is executed, so that the empirical averages $\phat_i^r$ are defined for all arms at all rounds (even for arms that were eliminated prior to some round).
Since $\phat_i^r$ is the empirical average of $k t_r$ samples of arm $i$ (aggregated from all players),
for any round $r$ and arm $i$ we have by Hoeffding's inequality,
\begin{align*}
	\Pr[\abs{\phat_i^r - p_i} \ge \eps_r/2]
	\le 2 \exp(-\eps_r^2 k t_r/2)
	= \del / 2n r^2 \,.
\end{align*}
%\rl{where was $\del_r$ defined?} \tk{typo - fixed}
Hence, a union bound gives that $\abs{\phat_i^r - p_i} < \eps_r/2$ for all $i$ and $r$ with probability at least
\begin{align*}
	1 - \sum_{r=1}^{\infty} \sum_{i=1}^{n} \frac{\del}{2n r^2}
	= 1 - \sum_{r=1}^{\infty} \frac{\del}{2 r^2}
	\ge 1-\del \,.
\end{align*}
That is, with probability at least $1-\del$, an $\eps$-optimal arm $i$ is never eliminated by the algorithm, as the event $\phat_i^r < \phat_{\st}^r - \eps_r$ implies that either $\phat_i^r < p_i - \eps_r/2$ or $\phat_j^r > p_j + \eps_r/2$ for some arm $j$.
In addition, any suboptimal arm $i$ does not survive round $r_i = \ceil{\log_2(1/\Dele_i)} + 1$, since $\Dele_i \ge 2 \eps_{r_i}$ and so for $r = r_i$,
%In addition, any arm $i$ with $\Del_i \ge 2\eps_r$ does not survive round $r$, since
\begin{align*}
	\phat_i^{r}
	< p_i + \eps_{r}/2 
	= p_1 + \eps_{r}/2 - \Del_i 
	\le \phat_1^{r} + \eps_{r} - \Del_i
	\le \phat_1^{r} - \eps_{r} 
	\le \phat_\st^{r} - \eps_{r} \,.
\end{align*}
That is, with probability at least $1-\del$, after $\ceil{\log_2(1/\eps)} + 1$ rounds (when the algorithm terminates) all remaining arms are $\eps$-optimal.  
When $\eps=0$, the algorithm terminates once only a single arm survives, and with high probability this occurs after at most $\ceil{\log_2(1/\Del)} + 1$ rounds.

%That is, with probability at least $1-\del$ the algorithm terminates after $\ceil{\log_2(1/\eps)} + 1$ rounds and returns an $\eps$-optimal.

We conclude by computing the total number of arms pulls required for the algorithm. 
Let $T_i$ be the total number of times arm~$i \ne 1$ is pulled by one of the players.
Since $r_i \le \log_2(4/\Dele_i)$, we have 
\begin{align*}
	T_i 
	&\le \frac{2}{k} \cdot (2^{r_i})^2 \ln \frac{4n r_i^2}{\del} \\
	&\le \frac{2}{k} \left( \frac{4}{\Dele_i} \right)^2 
		\ln \left( \frac{4n}{\del} \log_2^2 \frac{4}{\Dele_i} \right) \\
	&= \OO{
		\frac{1}{k} \cdot
		\frac{1}{(\Dele_i)^2} \log \left(\frac{n}{\del} \log \frac{1}{\Dele_i}\right)
		} .
\end{align*}
Consequently, the total number of arm pulls per player is $T_2 + \sum_{i=2}^{n} T_i$, which gives the theorem.
\end{proof}





\begin{proof}[Proof (of Corollary~\ref{cor:main3})]
For all arms $i \in [n]$, let
\begin{align*}
	r_i = \left\lceil
		R \, \frac{1+\log_2 (1/\Dele_i)}{\log_2 (1/\eps)}
	\right\rceil .
\end{align*}
Since $\Del_i \ge 2\eps_{r_i}$, if the algorithm is successful any arm $i$ which is not $\eps_{r_i}$-optimal is eliminated after at most $r_i$ rounds.
Clearly, after $R$ rounds only $\eps$-optimal arms survive.
This happens with probability at least $1-\del$.

It remains to bound the number of arm pulls the algorithm uses.
It is easy to verify that $2\eps_{r_i} \ge \eps^{1/R} \cdot \Dele_i$, thus the number of times arm $i$ was pulled by each of the players is
\begin{align*}
	T_i 
	= \frac{1}{k} \cdot \frac{4}{\eps_{r_i}^{2}} \ln \frac{2n r_i^2}{\del}
	= \OO{
		\frac{\eps^{-2/R}}{k} \cdot \frac{1}{(\Dele_i)^2} \log \frac{n R}{\del}
	} ,
\end{align*}
and the theorem follows.
\end{proof}



\fi
% end conf. omission


\end{document}  